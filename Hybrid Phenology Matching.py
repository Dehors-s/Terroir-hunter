import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
from scipy.interpolate import interp1d
import ee
import os
import geemap

# ==========================================
# è¾“å‡ºè®¾ç½®
# ==========================================
TOP_N = 10
SIM_THRESHOLD = 60.0
EXPORT_CSV = True
EXPORT_MAP = True
OUTPUT_CSV = 'similar_regions.csv'
OUTPUT_MAP = 'similar_regions_map.html'

# ==========================================
# 1. æ•°æ®æ¨¡æ‹Ÿ (Data Simulation)
# ==========================================
#ï¼š
# Reference: æ ‡å‡†çš„â€œæ³¢å°”å¤šâ€æ›²çº¿
# Target: å¾…æ£€æµ‹çš„è´«å›°æ‘æ›²çº¿ï¼ˆå‡è®¾å®ƒå‘èŠ½æ™šäº†10å¤©ï¼Œä½†ç”Ÿé•¿å½¢æ€é«˜åº¦ä¸€è‡´ï¼‰
def double_logistic(t, params):
    # åŒé€»è¾‘æ–¯è’‚å‡½æ•°æ¨¡æ‹Ÿä½œç‰©ç”Ÿé•¿
    # params: [t_start, t_end, rate_growth, rate_decay, base, amp]
    p = params
    growth = 1 / (1 + np.exp(-p[2] * (t - p[0])))
    decay = 1 / (1 + np.exp(p[3] * (t - p[1])))
    return p[4] + p[5] * (growth - (1 - decay)) # ç®€åŒ–çš„åŒé€»è¾‘æ–¯è’‚å½¢æ€

# ä½¿ç”¨ GEE æ•°æ®ä»£æ›¿æ¨¡æ‹Ÿæ•°æ®
# åˆå§‹åŒ– Earth Engineï¼ˆä½¿ç”¨ä¸ HAP ç›¸åŒçš„é¡¹ç›®ï¼‰
project_id = os.getenv('GCP_PROJECT_ID', 'terrior-hunter')
try:
    ee.Initialize(project=project_id)
except Exception:
    ee.Authenticate()
    ee.Initialize(project=project_id)

# å¯¼å…¥ HAP æ¨¡å—ï¼Œè·å–å·²è®¡ç®—çš„é€‚å®œæ€§å›¾å±‚å’Œ ROI
import AHP
final = AHP.final_suitability
roi = AHP.roi

print("\n" + "="*60)
print("å¯Œå£«è‹¹æœç›¸ä¼¼äº§åŒºæ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ")
print("="*60)
print("é‡‘æ ‡å‡†äº§åŒº: é™•è¥¿æ´›å· (109.4Â°E, 35.8Â°N)")
print("æœç´¢èŒƒå›´: 50km åŠå¾„å‘¨è¾¹åœ°åŒº")
print("ç›®æ ‡: æ‰¾åˆ°ä¸æ´›å·ç”Ÿæ€æ¡ä»¶æ¥è¿‘çš„æ½œåœ¨ä¼˜è´¨åœ°å—")
print("="*60 + "\n")

# æ‰¾åˆ°å¾—åˆ†>60 çš„é«˜åˆ†åŒºå¹¶å‘é‡åŒ–ï¼Œé€‰å–é¢ç§¯æœ€å¤§çš„å¤šè¾¹å½¢ä½œä¸ºå‚è€ƒåŒº
prime = final.gt(60).selfMask()
# å¢åŠ  scale å’Œ eightConnected å‚æ•°ï¼Œå‡å°‘è¦ç´ æ•°é‡ï¼ŒåŠ å¿«å¤„ç†
vec = prime.reduceToVectors(
    scale=1000,  # æé«˜åˆ†è¾¨ç‡ï¼Œå‡å°‘è¿‡åº¦åˆ†å‰²
    geometry=roi,
    geometryType='polygon',
    maxPixels=1e13,
    eightConnected=True  # ä½¿ç”¨8é‚»æ¥æ¥ç®€åŒ–è¦ç´ 
)
# æ·»åŠ é¢ç§¯å±æ€§åˆ° featureï¼Œè¿›è¡ŒæœåŠ¡å™¨ç«¯è®¡ç®—
vec_with_area = vec.map(lambda f: f.set('area', f.geometry().area(maxError=50)))
# æ’åºå¹¶é€‰å–é¢ç§¯æœ€å¤§çš„è¦ç´ 
best_feat = vec_with_area.sort('area', False).first()
if best_feat is None:
    raise RuntimeError('æœªæ‰¾åˆ°é«˜åˆ†åŒºåŸŸï¼Œè¯·ç¡®è®¤ HAP.py ä¸­ final_suitability æ˜¯å¦åŒ…å«å¾—åˆ†>60 çš„åŒºåŸŸã€‚')
best_geom = best_feat.geometry()

# å‡½æ•°ï¼šä» MODIS è·å–å¹´å†… NDVI æ—¶åºï¼ˆæŒ‰ MOD13Q1 16-dayï¼‰
def get_ndvi_series(geometry, year=2020):
    start = f"{year}-01-01"
    end = f"{year}-12-31"
    col = ee.ImageCollection('MODIS/006/MOD13Q1').filterDate(start, end).select('NDVI')
    
    # ä¸ºæ¯ä¸ªå½±åƒæ·»åŠ  NDVI å‡å€¼ä½œä¸ºå±æ€§
    def add_ndvi_property(img):
        ndvi_mean = img.reduceRegion(ee.Reducer.mean(), geometry, scale=250, bestEffort=True).get('NDVI')
        return img.set('ndvi_mean', ndvi_mean)
    
    col_with_ndvi = col.map(add_ndvi_property)
    
    # ç”¨ aggregate_array åˆ†åˆ«æå–æ—¶é—´æˆ³å’Œ NDVI å€¼
    dates = col_with_ndvi.aggregate_array('system:time_start').getInfo()
    values = col_with_ndvi.aggregate_array('ndvi_mean').getInfo()
    
    if not dates or not values or len(dates) == 0:
        return np.full(365, np.nan)
    
    # æ ‡å‡†åŒ–ä¸º DOY ç´¢å¼•é•¿åº¦
    from datetime import datetime
    doys = []
    clean_values = []
    
    for d, v in zip(dates, values):
        if isinstance(d, (int, float)) and d is not None:
            try:
                doy = datetime.utcfromtimestamp(d/1000).timetuple().tm_yday
                if v is not None:
                    clean_values.append(float(v) * 0.0001)
                    doys.append(doy)
                else:
                    clean_values.append(np.nan)
                    doys.append(doy)
            except:
                pass
    
    if len([v for v in clean_values if not np.isnan(v)]) < 3:
        return np.full(365, np.nan)
    
    x = np.array(doys)
    y = np.array(clean_values)
    valid_idx = ~np.isnan(y)
    
    if valid_idx.sum() < 3:
        return np.full(365, np.nan)
    
    f = interp1d(x[valid_idx], y[valid_idx], kind='linear', fill_value='extrapolate')
    full = f(np.arange(1, 366))
    return full

# å‚è€ƒæ ·æœ¬ï¼šä½¿ç”¨æœ€ä½³é«˜åˆ†åŒºçš„ NDVI åºåˆ—
ref_ndvi = get_ndvi_series(best_geom, year=2020)

# åœ¨ ROI å†…é‡‡æ ·è‹¥å¹²ç‚¹ä½œä¸ºå€™é€‰ç›®æ ‡ï¼ˆå‡å°‘é‡‡æ ·ç‚¹æ•°ä»¥åŠ å¿«å¤„ç†ï¼‰
sample_pts = final.sample(
    region=roi,
    scale=500,
    numPixels=20,  # å‡å°‘é‡‡æ ·ç‚¹æ•°ä» 50 åˆ° 20
    geometries=True
).getInfo().get('features', [])

# ä½œä¸ºåç»­åŒ¹é…ç”¨çš„æ—¶é—´è½´
days = np.arange(1, 366)

# ==========================================
# 2. ç‰¹å¾æå–ï¼šå¯»æ‰¾â€œå…³èŠ‚â€ (Landmark Extraction)
# ==========================================
def clean_ndvi_series(ndvi_series):
    series = np.array(ndvi_series, dtype=float)
    if np.all(np.isnan(series)):
        return None
    valid = ~np.isnan(series)
    if valid.sum() < 3:
        return None
    x = np.arange(series.size)
    series[~valid] = np.interp(x[~valid], x[valid], series[valid])
    return series

def extract_landmarks(ndvi_series):
    """
    æ ¹æ®è®ºæ–‡æ–¹æ³•ï¼Œåˆ©ç”¨æ›²ç‡å˜åŒ–ç‡(Rate of Change of Curvature)æå–å…³é”®ç‚¹
    è¿™é‡Œä½¿ç”¨ Savitzky-Golay æ»¤æ³¢å¹³æ»‘å¹¶è®¡ç®—å¯¼æ•°
    """
    cleaned = clean_ndvi_series(ndvi_series)
    if cleaned is None:
        return None, None

    # a. å¹³æ»‘å»å™ª (Smoothing)
    smooth_ndvi = savgol_filter(cleaned, window_length=31, polyorder=3)
    
    # b. è®¡ç®—å¯¼æ•° (Derivatives)
    d1 = np.gradient(smooth_ndvi) # ä¸€é˜¶å¯¼æ•° (ç”Ÿé•¿é€Ÿåº¦)
    d2 = np.gradient(d1)          # äºŒé˜¶å¯¼æ•° (åŠ é€Ÿåº¦)
    d3 = np.gradient(d2)          # ä¸‰é˜¶å¯¼æ•° (æ›²ç‡å˜åŒ–ç‡è¿‘ä¼¼)
    
    # c. å¯»æ‰¾å…³é”®ç‚¹ (Landmarks)
    # è®ºæ–‡ä¸­ï¼šGreenup/Maturity æ˜¯æ›²ç‡å˜åŒ–ç‡çš„å±€éƒ¨æœ€å¤§å€¼
    #        Senescence/Dormancy æ˜¯æ›²ç‡å˜åŒ–ç‡çš„å±€éƒ¨æœ€å°å€¼
    # è¿™é‡Œç®€åŒ–é€»è¾‘ï¼šå¯»æ‰¾ d3 çš„æå€¼ç‚¹ä½œä¸ºè¿‘ä¼¼ç‰¹å¾
    
    # ç®€å•çš„å³°å€¼æ£€æµ‹
    from scipy.signal import find_peaks
    # ç”Ÿé•¿æœŸ (Upward): æ‰¾ d3 çš„æ­£å³°å€¼
    upward_peaks, _ = find_peaks(d3[:180], height=0.0001, distance=20) 
    # è¡°é€€æœŸ (Downward): æ‰¾ d3 çš„è´Ÿå³°å€¼ (å³ -d3 çš„æ­£å³°å€¼)
    downward_peaks, _ = find_peaks(-d3[180:], height=0.0001, distance=20)
    downward_peaks += 180 # ä¿®æ­£ç´¢å¼•
    
    # é€‰å–æœ€é‡è¦çš„4ä¸ªç‚¹ (å‡è®¾)
    # Greenup (è¿”é’), Maturity (æˆç†Ÿ), Senescence (è¡°è€), Dormancy (ä¼‘çœ )
    try:
        landmarks = {
            'Greenup': upward_peaks[0],
            'Maturity': upward_peaks[-1], # å‡è®¾æœ€åä¸€ä¸ªä¸Šå‡å³°å€¼æ˜¯æˆç†Ÿå‰å¥
            'Senescence': downward_peaks[0],
            'Dormancy': downward_peaks[-1]
        }
    except:
        # å…œåº•ï¼šå¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å®Œç¾å³°å€¼ï¼Œä½¿ç”¨ç®€å•çš„é˜ˆå€¼æ³•æˆ–å›ºå®šç‚¹
        landmarks = {'Greenup': 100, 'Maturity': 150, 'Senescence': 260, 'Dormancy': 300}
        
    return smooth_ndvi, landmarks

# ==========================================
# 3. æ··åˆåŒ¹é…ï¼šMICA ç®—æ³• (Phenophase Matching)
# ==========================================
def warp_and_match(ref_curve, tgt_curve, ref_lm, tgt_lm):
    """
    åˆ©ç”¨ MICA (Multi-Interval Curve Alignment) å¯¹é½æ›²çº¿
    """
    # a. å¯¹é½å…³é”®ç‚¹ (Align Landmarks)
    # å°† Target çš„å…³é”®ç‚¹æ˜ å°„åˆ° Reference çš„å…³é”®ç‚¹ä½ç½®
    # æ„å»ºç®€å•çš„åˆ†æ®µçº¿æ€§æ˜ å°„å‡½æ•° A(x)
    
    key_points_ref = sorted(list(ref_lm.values()))
    key_points_tgt = sorted(list(tgt_lm.values()))
    
    # æ·»åŠ èµ·å§‹å’Œç»“æŸç‚¹ (0, 365) ä¿è¯å…¨è¦†ç›–
    x_ref = [0] + key_points_ref + [365]
    x_tgt = [0] + key_points_tgt + [365]
    
    # å»ºç«‹æ˜ å°„å…³ç³»: Target time -> Reference time
    warp_func = interp1d(x_tgt, x_ref, kind='linear', fill_value="extrapolate")
    
    # b. æ‰­æ›²ç›®æ ‡æ›²çº¿ (Warp Target Curve)
    # è®¡ç®—â€œæ ¡æ­£åâ€çš„ Target æ›²çº¿åœ¨æ ‡å‡†æ—¶é—´è½´ä¸Šçš„å½¢æ€
    warped_time = warp_func(np.arange(len(tgt_curve)))
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯æƒ³çœ‹ Target åœ¨ Ref æ—¶é—´è½´ä¸Šçš„è¡¨ç°
    # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°† Target çš„å€¼â€œç§»â€åˆ° Ref çš„æ—¶é—´ç‚¹ä¸Š
    
    # åå‘æ’å€¼ï¼šåœ¨ Ref çš„æ—¶é—´ç½‘æ ¼ä¸Šï¼Œæ‰¾åˆ°å¯¹åº”çš„ Target å€¼
    # çœŸå®çš„ MICA æ›´å¤æ‚ï¼Œè¿™é‡Œåšæ¼”ç¤ºçº§ç®€åŒ–
    inverse_warp = interp1d(x_ref, x_tgt, kind='linear', fill_value="extrapolate")
    original_time_indices = inverse_warp(np.arange(len(ref_curve)))
    # é™åˆ¶ç´¢å¼•èŒƒå›´
    original_time_indices = np.clip(original_time_indices, 0, 364)
    
    warped_tgt_curve = interp1d(np.arange(len(tgt_curve)), tgt_curve)(original_time_indices)
    
    return warped_tgt_curve

# ==========================================
# 4. ç›¸ä¼¼åº¦è®¡ç®—ï¼šåŸºäºæ–œç‡è·ç¦» (Slope-based Distance)
# ==========================================
def calculate_similarity(curve1, curve2):
    """
    è®ºæ–‡å…¬å¼ (1): åŸºäºæ–œç‡çš„è·ç¦»å‡½æ•°
    d(Ct, Cr) = mean( |slope_t - slope_r| )
    """
    # è®¡ç®—æ–œç‡ (Slope)
    s1 = np.gradient(curve1)
    s2 = np.gradient(curve2)
    
    # è®¡ç®—è·ç¦» (Distance)
    # è·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
    dist = np.mean(np.abs(s1 - s2))
    
    # è½¬æ¢ä¸º 0-100 çš„ç›¸ä¼¼åº¦åˆ†æ•° (heuristic)
    similarity = 100 * np.exp(-10 * dist) 
    return similarity, dist

# ==========================================
# 5. ç›¸ä¼¼åŒºåŸŸæ£€ç´¢ (Similarity Search)
# ==========================================
ref_smooth, ref_marks = extract_landmarks(ref_ndvi)
if ref_smooth is None:
    raise RuntimeError('å‚è€ƒåŒº NDVI æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…ã€‚')

results = []
for f in sample_pts:
    geom = ee.Geometry(f['geometry'])
    ndvi_ts = get_ndvi_series(geom, year=2020)
    smooth, marks = extract_landmarks(ndvi_ts)
    if smooth is None:
        continue
    warped = warp_and_match(ref_smooth, smooth, ref_marks, marks)
    sim_score, raw_dist = calculate_similarity(ref_smooth, warped)
    if sim_score < SIM_THRESHOLD:
        continue
    centroid = geom.centroid(maxError=1).coordinates().getInfo()
    results.append({
        'geometry': f['geometry'],
        'similarity': sim_score,
        'distance': raw_dist,
        'centroid': centroid,
        'smooth': smooth,
        'warped': warped,
        'landmarks': marks
    })

results.sort(key=lambda x: x['similarity'], reverse=True)
top_results = results[:TOP_N]

if not top_results:
    raise RuntimeError('æœªæ‰¾åˆ°æ»¡è¶³é˜ˆå€¼çš„ç›¸ä¼¼åŒºåŸŸï¼Œè¯·é™ä½é˜ˆå€¼æˆ–å¢åŠ é‡‡æ ·ç‚¹æ•°ã€‚')

if EXPORT_CSV:
    rows = []
    for idx, r in enumerate(top_results, start=1):
        rows.append({
            'rank': idx,
            'similarity': round(r['similarity'], 3),
            'distance': round(r['distance'], 6),
            'lon': r['centroid'][0],
            'lat': r['centroid'][1]
        })
    pd.DataFrame(rows).to_csv(OUTPUT_CSV, index=False)
    print(f"âœ“ ç›¸ä¼¼åŒºåŸŸç»“æœå·²ä¿å­˜åˆ°: {OUTPUT_CSV}")

if EXPORT_MAP:
    # åœ°å›¾ä¸­å¿ƒè®¾ä¸ºæ´›å·äº§åŒº
    Map = geemap.Map(center=[35.8, 109.4], zoom=10, basemap='SATELLITE')
    
    # æ·»åŠ é‡‘æ ‡å‡†äº§åŒºï¼ˆè“è‰²ï¼‰
    ref_fc = ee.FeatureCollection([ee.Feature(best_geom)])
    ref_styled = ref_fc.style(**{
        'color': '#0066ff',
        'width': 3,
        'fillColor': '#0066ff33'
    })
    Map.addLayer(ref_styled, {}, 'ğŸ† é‡‘æ ‡å‡†äº§åŒº (æ´›å·)')
    
    # æ·»åŠ ç›¸ä¼¼äº§åŒºï¼ˆæ©™è‰²ç‚¹ï¼‰
    top_fc = ee.FeatureCollection([
        ee.Feature(ee.Geometry(r['geometry']), {'similarity': r['similarity']})
        for r in top_results
    ])
    styled = top_fc.style(**{
        'color': '#ff6600',
        'pointSize': 8,
        'width': 2,
        'fillColor': '#ffcc00'
    })
    Map.addLayer(styled, {}, f'ğŸ“ ç›¸ä¼¼äº§åŒº (Top {len(top_results)})')
    
    Map.to_html(OUTPUT_MAP)
    print(f"âœ“ ç›¸ä¼¼åŒºåŸŸåœ°å›¾å·²ä¿å­˜åˆ°: {OUTPUT_MAP}")
    print(f"  - é‡‘æ ‡å‡†äº§åŒº: è“è‰²åŒºåŸŸ")
    print(f"  - ç›¸ä¼¼äº§åŒº: æ©™è‰²ç‚¹æ ‡æ³¨ (å…± {len(top_results)} ä¸ª)")
    print(f"  - ç›¸ä¼¼åº¦èŒƒå›´: {top_results[-1]['similarity']:.1f}% - {top_results[0]['similarity']:.1f}%")

print("\n" + "="*60)
print("âœ… åˆ†æå®Œæˆï¼")
print("="*60)

# é€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„åŒºåŸŸè¿›è¡Œå¯è§†åŒ–å¯¹æ¯”
best_match = top_results[0]
tgt_smooth = best_match['smooth']
tgt_marks = best_match['landmarks']
warped_tgt = best_match['warped']
sim_score = best_match['similarity']

# ==========================================
# 6. ç»“æœå¯è§†åŒ– (Visualization)
# ==========================================
print("\nğŸ“Š ç”Ÿæˆæ›²çº¿å¯¹æ¯”å›¾...\n")

fig = plt.figure(figsize=(15, 10))

# å­å›¾1: é‡‘æ ‡å‡†äº§åŒº NDVI æ—¶åº
ax1 = plt.subplot(2, 2, 1)
ax1.plot(days, ref_smooth, 'b-', linewidth=2.5)
ax1.fill_between(days, ref_smooth, alpha=0.2, color='blue')
ax1.scatter(ref_marks.values(), [ref_smooth[int(i)] for i in ref_marks.values()], 
           c='darkblue', s=80, zorder=5, marker='o')
for name, idx in ref_marks.items():
    ax1.annotate(name, (idx, ref_smooth[int(idx)]), 
                xytext=(5, 5), textcoords='offset points', fontsize=9)
ax1.set_title('ğŸ† é‡‘æ ‡å‡†äº§åŒº (æ´›å·) NDVI æ—¶åº', fontsize=12, fontweight='bold')
ax1.set_xlabel('Day of Year (DOY)')
ax1.set_ylabel('NDVI')
ax1.grid(True, alpha=0.3)

# å­å›¾2: æœ€ç›¸ä¼¼äº§åŒº NDVI æ—¶åº
ax2 = plt.subplot(2, 2, 2)
ax2.plot(days, tgt_smooth, 'r-', linewidth=2.5)
ax2.fill_between(days, tgt_smooth, alpha=0.2, color='red')
ax2.scatter(tgt_marks.values(), [tgt_smooth[int(i)] for i in tgt_marks.values()], 
           c='darkred', s=80, zorder=5, marker='s')
for name, idx in tgt_marks.items():
    ax2.annotate(name, (idx, tgt_smooth[int(idx)]), 
                xytext=(5, 5), textcoords='offset points', fontsize=9)
ax2.set_title(f'ğŸ“ æœ€ç›¸ä¼¼äº§åŒº (ç›¸ä¼¼åº¦: {sim_score:.1f}%) NDVI æ—¶åº', fontsize=12, fontweight='bold')
ax2.set_xlabel('Day of Year (DOY)')
ax2.set_ylabel('NDVI')
ax2.grid(True, alpha=0.3)

# å­å›¾3: æ—¶é—´å¯¹é½å‰çš„æ›²çº¿å¯¹æ¯”
ax3 = plt.subplot(2, 2, 3)
ax3.plot(days, ref_smooth, 'b-', label='é‡‘æ ‡å‡†äº§åŒº (æ´›å·)', linewidth=2.5)
ax3.plot(days, tgt_smooth, 'r--', label='å€™é€‰äº§åŒº', linewidth=2.5)
ax3.scatter(ref_marks.values(), [ref_smooth[int(i)] for i in ref_marks.values()], 
           c='blue', s=60, zorder=5, marker='o', alpha=0.7)
ax3.scatter(tgt_marks.values(), [tgt_smooth[int(i)] for i in tgt_marks.values()], 
           c='red', s=60, zorder=5, marker='s', alpha=0.7)
ax3.set_title('å¯¹é½å‰: æ—¶é—´åç§»æ˜æ˜¾', fontsize=12, fontweight='bold')
ax3.set_xlabel('Day of Year (DOY)')
ax3.set_ylabel('NDVI')
ax3.legend(loc='best', fontsize=10)
ax3.grid(True, alpha=0.3)

# å­å›¾4: æ—¶é—´å¯¹é½åçš„æ›²çº¿å¯¹æ¯”
ax4 = plt.subplot(2, 2, 4)
ax4.plot(days, ref_smooth, 'b-', label='é‡‘æ ‡å‡†äº§åŒº (æ´›å·)', linewidth=2.5)
ax4.plot(days, warped_tgt, 'g-', label='å€™é€‰äº§åŒº (å¯¹é½å)', linewidth=2.5)
ax4.fill_between(days, ref_smooth, warped_tgt, alpha=0.1, color='gray')
ax4.set_title(f'âœ… å¯¹é½å: ç‰©å€™ä¸€è‡´ (ç›¸ä¼¼åº¦: {sim_score:.1f}%)', fontsize=12, fontweight='bold')
ax4.set_xlabel('Day of Year (æ ‡å‡†åŒ–)')
ax4.set_ylabel('NDVI')
ax4.legend(loc='best', fontsize=10)
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('phenology_matching_analysis.png', dpi=150, bbox_inches='tight')
print("âœ“ æ›²çº¿å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: phenology_matching_analysis.png\n")
plt.show()